{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUehXgCyIRdq"
      },
      "source": [
        "# Actividad - Proyecto práctico\n",
        "\n",
        "\n",
        "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
        "*   Alumno 1: David Fernando Pozo Espín\n",
        "*   Alumno 2: Steven Alberto Mena Chavez\n",
        "*   Alumno 3:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwpYlnjWJhS9"
      },
      "source": [
        "---\n",
        "## **PARTE 1** - Instalación y requisitos previos\n",
        "\n",
        "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
        "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
        "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
        "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
        "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU2BPrK2JkP0"
      },
      "source": [
        "---\n",
        "### 1.1. Preparar enviroment (solo local)\n",
        "\n",
        "\n",
        "\n",
        "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
        "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
        "2. Instalar Anaconda\n",
        "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
        "\n",
        "\n",
        "```\n",
        "conda create --name miar_rl python=3.8\n",
        "conda activate miar_rl\n",
        "cd \"PATH_TO_FOLDER\"\n",
        "conda install git\n",
        "pip install jupyter\n",
        "```\n",
        "\n",
        "\n",
        "4. Abrir la notebook con *jupyter-notebook*.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "jupyter-notebook\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-kixNPiJqTc"
      },
      "source": [
        "---\n",
        "### 1.2. Localizar entorno de trabajo: Google colab o local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YDFwZ-JscI"
      },
      "outputs": [],
      "source": [
        "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
        "mount='/content/gdrive'\n",
        "drive_root = mount + \"/My Drive/08_MIAR/actividades/proyecto practico\"\n",
        "\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  IN_COLAB=True\n",
        "except:\n",
        "  IN_COLAB=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dp_a1iBJ0tf"
      },
      "source": [
        "---\n",
        "### 1.3. Montar carpeta de datos local (solo Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6n7MIefJ21i",
        "outputId": "1bdf7934-c6b2-45cd-bc9e-10e454fe5328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos en el directorio: \n",
            "['.ipynb_checkpoints', '08MIAR_Aprendizaje_por_refuerzo_Proyecto_práctico.ipynb', 'checkpoint', 'dqn_SpaceInvaders-v0_log.json', 'dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights.h5f.index', 'dqn_SpaceInvaders-v0_weights_100000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_100000.h5f.index', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_1000000.h5f.index', 'dqn_SpaceInvaders-v0_weights_200000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_200000.h5f.index', 'dqn_SpaceInvaders-v0_weights_300000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_300000.h5f.index', 'dqn_SpaceInvaders-v0_weights_400000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_400000.h5f.index', 'dqn_SpaceInvaders-v0_weights_500000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_500000.h5f.index', 'dqn_SpaceInvaders-v0_weights_600000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_600000.h5f.index', 'dqn_SpaceInvaders-v0_weights_700000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_700000.h5f.index', 'dqn_SpaceInvaders-v0_weights_800000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_800000.h5f.index', 'dqn_SpaceInvaders-v0_weights_900000.h5f.data-00000-of-00001', 'dqn_SpaceInvaders-v0_weights_900000.h5f.index']\n"
          ]
        }
      ],
      "source": [
        "# Switch to the directory on the Google Drive that you want to use\n",
        "import os\n",
        "if IN_COLAB:\n",
        "  print(\"We're running Colab\")\n",
        "\n",
        "  if IN_COLAB:\n",
        "    # Mount the Google Drive at mount\n",
        "    print(\"Colab: mounting Google drive on \", mount)\n",
        "\n",
        "    drive.mount(mount)\n",
        "\n",
        "    # Create drive_root if it doesn't exist\n",
        "    create_drive_root = True\n",
        "    if create_drive_root:\n",
        "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
        "      os.makedirs(drive_root, exist_ok=True)\n",
        "\n",
        "    # Change to the directory\n",
        "    print(\"\\nColab: Changing directory to \", drive_root)\n",
        "    %cd $drive_root\n",
        "# Verify we're in the correct working directory\n",
        "%pwd\n",
        "print(\"Archivos en el directorio: \")\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1ZSL5bpJ560"
      },
      "source": [
        "---\n",
        "### 1.4. Instalar librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbVRjvHCJ8UF"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install tensorflow==2.8\n",
        "else:\n",
        "  %pip install gym==0.17.3\n",
        "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
        "  %pip install pyglet==1.5.0\n",
        "  %pip install h5py==3.1.0\n",
        "  %pip install Pillow==9.5.0\n",
        "  %pip install keras-rl2==1.0.5\n",
        "  %pip install Keras==2.2.4\n",
        "  %pip install tensorflow==2.5.3\n",
        "  #%pip install torch==2.0.1\n",
        "  %pip install agents==1.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hzP_5ZuGb2X"
      },
      "source": [
        "---\n",
        "## **PARTE 2**. Enunciado\n",
        "\n",
        "Consideraciones a tener en cuenta:\n",
        "\n",
        "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
        "\n",
        "- Para nuestro ejercicio, una solución óptima será alcanzada cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
        "\n",
        "Este proyecto práctico consta de tres partes:\n",
        "\n",
        "1.   Implementar la red neuronal que se usará en la solución\n",
        "2.   Implementar las distintas piezas de la solución DQN\n",
        "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
        "\n",
        "IMPORTANTE:\n",
        "\n",
        "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
        "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_b3mzw8IzJP"
      },
      "source": [
        "---\n",
        "## **PARTE 3**. Desarrollo y preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duPmUNOVGb2a"
      },
      "source": [
        "#### Importar librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3eRhgI-Gb2a"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4jgQjzoGb2a"
      },
      "source": [
        "#### Configuración base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwOE6I_KGb2a"
      },
      "outputs": [],
      "source": [
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "env_name = 'SpaceInvaders-v0'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGEZUcpGb2a"
      },
      "outputs": [],
      "source": [
        "class AtariProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yitXTADGb2b"
      },
      "source": [
        "1. Implementación de la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4GKrfWSGb2b",
        "outputId": "63644f07-0d11-4a76-e8da-4c5b261db473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n"
          ]
        }
      ],
      "source": [
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP2Svqi5TfhW",
        "outputId": "d5ab7d5f-fb5c-4ca5-9c3a-98958d32648c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "channels_last\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute (Permute)            (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 80, 80, 16)        1616      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 40, 40, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 38, 38, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 19, 19, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 17, 17, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 774       \n",
            "=================================================================\n",
            "Total params: 2,188,854\n",
            "Trainable params: 2,188,854\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "print(K.image_data_format())\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    # (width, height, channels)\n",
        "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
        "elif K.image_data_format() == 'channels_first':\n",
        "    # (channels, width, height)\n",
        "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
        "else:\n",
        "    raise RuntimeError('Unknown image_dim_ordering.')\n",
        "\n",
        "model.add(Convolution2D(16, (5, 5), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(layers.Dropout((0.2)))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(layers.Dropout((0.2)))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB9-_5HPGb2b"
      },
      "source": [
        "2. Implementación de la solución DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foSlxWH1Gb2b"
      },
      "outputs": [],
      "source": [
        "memory = SequentialMemory(limit=750000, window_length=WINDOW_LENGTH)\n",
        "processor = AtariProcessor()\n",
        "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
        "                              value_max=1., value_min=.1, value_test=.05,\n",
        "                              nb_steps=750000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQxHeYjyTfhZ",
        "outputId": "816d78b4-71cc-4d7a-d03b-def6a0217bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
          ]
        }
      ],
      "source": [
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
        "               memory=memory, processor=processor,\n",
        "               nb_steps_warmup=50000, gamma=.99,\n",
        "               target_model_update=10000,\n",
        "               train_interval=4)\n",
        "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOrOLvFZTfha",
        "outputId": "1e300965-981b-4f41-8c57-1ed51b319f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\david\\anaconda3\\envs\\VIU_Maestria\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 100s 10ms/step - reward: 0.0119\n",
            "15 episodes - episode_reward: 7.400 [4.000, 11.000] - ale.lives: 2.120\n",
            "\n",
            "Interval 2 (10000 steps performed)\n",
            "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0146\n",
            "13 episodes - episode_reward: 11.154 [6.000, 27.000] - ale.lives: 2.106\n",
            "\n",
            "Interval 3 (20000 steps performed)\n",
            "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0129\n",
            "16 episodes - episode_reward: 8.375 [3.000, 15.000] - ale.lives: 2.194\n",
            "\n",
            "Interval 4 (30000 steps performed)\n",
            "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0127\n",
            "13 episodes - episode_reward: 9.538 [2.000, 17.000] - ale.lives: 2.039\n",
            "\n",
            "Interval 5 (40000 steps performed)\n",
            "10000/10000 [==============================] - 98s 10ms/step - reward: 0.0153\n",
            "11 episodes - episode_reward: 14.091 [5.000, 26.000] - ale.lives: 2.131\n",
            "\n",
            "Interval 6 (50000 steps performed)\n",
            "10000/10000 [==============================] - 1037s 104ms/step - reward: 0.0145\n",
            "15 episodes - episode_reward: 9.933 [4.000, 27.000] - loss: 0.007 - mae: 0.092 - mean_q: 0.121 - mean_eps: 0.934 - ale.lives: 2.069\n",
            "\n",
            "Interval 7 (60000 steps performed)\n",
            "10000/10000 [==============================] - 1021s 102ms/step - reward: 0.0153\n",
            "16 episodes - episode_reward: 9.438 [5.000, 24.000] - loss: 0.007 - mae: 0.106 - mean_q: 0.135 - mean_eps: 0.922 - ale.lives: 2.116\n",
            "\n",
            "Interval 8 (70000 steps performed)\n",
            "10000/10000 [==============================] - 3044s 304ms/step - reward: 0.0108\n",
            "15 episodes - episode_reward: 6.933 [2.000, 12.000] - loss: 0.006 - mae: 0.130 - mean_q: 0.163 - mean_eps: 0.910 - ale.lives: 2.087\n",
            "\n",
            "Interval 9 (80000 steps performed)\n",
            "10000/10000 [==============================] - 422s 42ms/step - reward: 0.0158\n",
            "13 episodes - episode_reward: 11.846 [4.000, 27.000] - loss: 0.006 - mae: 0.138 - mean_q: 0.171 - mean_eps: 0.898 - ale.lives: 2.059\n",
            "\n",
            "Interval 10 (90000 steps performed)\n",
            "10000/10000 [==============================] - 427s 43ms/step - reward: 0.0121\n",
            "12 episodes - episode_reward: 10.583 [5.000, 18.000] - loss: 0.006 - mae: 0.149 - mean_q: 0.183 - mean_eps: 0.886 - ale.lives: 1.979\n",
            "\n",
            "Interval 11 (100000 steps performed)\n",
            "10000/10000 [==============================] - 421s 42ms/step - reward: 0.0159\n",
            "14 episodes - episode_reward: 11.214 [5.000, 30.000] - loss: 0.006 - mae: 0.152 - mean_q: 0.186 - mean_eps: 0.874 - ale.lives: 2.058\n",
            "\n",
            "Interval 12 (110000 steps performed)\n",
            "10000/10000 [==============================] - 426s 43ms/step - reward: 0.0118\n",
            "15 episodes - episode_reward: 8.200 [2.000, 16.000] - loss: 0.007 - mae: 0.176 - mean_q: 0.214 - mean_eps: 0.862 - ale.lives: 2.008\n",
            "\n",
            "Interval 13 (120000 steps performed)\n",
            "10000/10000 [==============================] - 441s 44ms/step - reward: 0.0122\n",
            "15 episodes - episode_reward: 8.200 [3.000, 13.000] - loss: 0.007 - mae: 0.185 - mean_q: 0.226 - mean_eps: 0.850 - ale.lives: 2.165\n",
            "\n",
            "Interval 14 (130000 steps performed)\n",
            "10000/10000 [==============================] - 450s 45ms/step - reward: 0.0142\n",
            "13 episodes - episode_reward: 9.615 [2.000, 19.000] - loss: 0.007 - mae: 0.192 - mean_q: 0.234 - mean_eps: 0.838 - ale.lives: 2.069\n",
            "\n",
            "Interval 15 (140000 steps performed)\n",
            "10000/10000 [==============================] - 460s 46ms/step - reward: 0.0125\n",
            "14 episodes - episode_reward: 10.214 [3.000, 20.000] - loss: 0.007 - mae: 0.204 - mean_q: 0.249 - mean_eps: 0.826 - ale.lives: 2.078\n",
            "\n",
            "Interval 16 (150000 steps performed)\n",
            "10000/10000 [==============================] - 474s 47ms/step - reward: 0.0141\n",
            "15 episodes - episode_reward: 9.000 [4.000, 16.000] - loss: 0.007 - mae: 0.228 - mean_q: 0.278 - mean_eps: 0.814 - ale.lives: 2.204\n",
            "\n",
            "Interval 17 (160000 steps performed)\n",
            "10000/10000 [==============================] - 497s 50ms/step - reward: 0.0142\n",
            "14 episodes - episode_reward: 10.143 [4.000, 18.000] - loss: 0.007 - mae: 0.256 - mean_q: 0.311 - mean_eps: 0.802 - ale.lives: 2.035\n",
            "\n",
            "Interval 18 (170000 steps performed)\n",
            "10000/10000 [==============================] - 506s 51ms/step - reward: 0.0142\n",
            "14 episodes - episode_reward: 9.786 [5.000, 18.000] - loss: 0.008 - mae: 0.269 - mean_q: 0.327 - mean_eps: 0.790 - ale.lives: 1.972\n",
            "\n",
            "Interval 19 (180000 steps performed)\n",
            "10000/10000 [==============================] - 511s 51ms/step - reward: 0.0141\n",
            "14 episodes - episode_reward: 10.857 [5.000, 19.000] - loss: 0.008 - mae: 0.295 - mean_q: 0.357 - mean_eps: 0.778 - ale.lives: 2.058\n",
            "\n",
            "Interval 20 (190000 steps performed)\n",
            "10000/10000 [==============================] - 524s 52ms/step - reward: 0.0156\n",
            "15 episodes - episode_reward: 10.400 [4.000, 20.000] - loss: 0.007 - mae: 0.304 - mean_q: 0.368 - mean_eps: 0.766 - ale.lives: 2.008\n",
            "\n",
            "Interval 21 (200000 steps performed)\n",
            "10000/10000 [==============================] - 547s 55ms/step - reward: 0.0154\n",
            "14 episodes - episode_reward: 11.000 [6.000, 24.000] - loss: 0.007 - mae: 0.319 - mean_q: 0.386 - mean_eps: 0.754 - ale.lives: 2.010\n",
            "\n",
            "Interval 22 (210000 steps performed)\n",
            "10000/10000 [==============================] - 590s 59ms/step - reward: 0.0143\n",
            "15 episodes - episode_reward: 9.200 [4.000, 34.000] - loss: 0.007 - mae: 0.326 - mean_q: 0.394 - mean_eps: 0.742 - ale.lives: 2.046\n",
            "\n",
            "Interval 23 (220000 steps performed)\n",
            "10000/10000 [==============================] - 624s 62ms/step - reward: 0.0157\n",
            "17 episodes - episode_reward: 9.471 [3.000, 21.000] - loss: 0.008 - mae: 0.349 - mean_q: 0.422 - mean_eps: 0.730 - ale.lives: 2.045\n",
            "\n",
            "Interval 24 (230000 steps performed)\n",
            "10000/10000 [==============================] - 647s 65ms/step - reward: 0.0161\n",
            "15 episodes - episode_reward: 10.467 [3.000, 20.000] - loss: 0.008 - mae: 0.364 - mean_q: 0.440 - mean_eps: 0.718 - ale.lives: 2.074\n",
            "\n",
            "Interval 25 (240000 steps performed)\n",
            "10000/10000 [==============================] - 663s 66ms/step - reward: 0.0174\n",
            "14 episodes - episode_reward: 11.214 [7.000, 19.000] - loss: 0.008 - mae: 0.374 - mean_q: 0.451 - mean_eps: 0.706 - ale.lives: 2.092\n",
            "\n",
            "Interval 26 (250000 steps performed)\n",
            "10000/10000 [==============================] - 629s 63ms/step - reward: 0.0157\n",
            "13 episodes - episode_reward: 13.000 [4.000, 28.000] - loss: 0.008 - mae: 0.388 - mean_q: 0.468 - mean_eps: 0.694 - ale.lives: 2.047\n",
            "\n",
            "Interval 27 (260000 steps performed)\n",
            "10000/10000 [==============================] - 587s 59ms/step - reward: 0.0179\n",
            "14 episodes - episode_reward: 12.786 [4.000, 28.000] - loss: 0.009 - mae: 0.427 - mean_q: 0.516 - mean_eps: 0.682 - ale.lives: 2.028\n",
            "\n",
            "Interval 28 (270000 steps performed)\n",
            "10000/10000 [==============================] - 692s 69ms/step - reward: 0.0166\n",
            "17 episodes - episode_reward: 10.353 [4.000, 21.000] - loss: 0.009 - mae: 0.436 - mean_q: 0.525 - mean_eps: 0.670 - ale.lives: 2.076\n",
            "\n",
            "Interval 29 (280000 steps performed)\n",
            "10000/10000 [==============================] - 705s 70ms/step - reward: 0.0165\n",
            "15 episodes - episode_reward: 10.000 [2.000, 21.000] - loss: 0.010 - mae: 0.461 - mean_q: 0.556 - mean_eps: 0.658 - ale.lives: 2.121\n",
            "\n",
            "Interval 30 (290000 steps performed)\n",
            "10000/10000 [==============================] - 712s 71ms/step - reward: 0.0152\n",
            "16 episodes - episode_reward: 10.312 [4.000, 20.000] - loss: 0.010 - mae: 0.468 - mean_q: 0.564 - mean_eps: 0.646 - ale.lives: 2.036\n",
            "\n",
            "Interval 31 (300000 steps performed)\n",
            "10000/10000 [==============================] - 719s 72ms/step - reward: 0.0165\n",
            "17 episodes - episode_reward: 9.647 [3.000, 20.000] - loss: 0.010 - mae: 0.491 - mean_q: 0.592 - mean_eps: 0.634 - ale.lives: 2.123\n",
            "\n",
            "Interval 32 (310000 steps performed)\n",
            "10000/10000 [==============================] - 700s 70ms/step - reward: 0.0190\n",
            "15 episodes - episode_reward: 12.400 [4.000, 23.000] - loss: 0.010 - mae: 0.512 - mean_q: 0.617 - mean_eps: 0.622 - ale.lives: 2.042\n",
            "\n",
            "Interval 33 (320000 steps performed)\n",
            "10000/10000 [==============================] - 696s 70ms/step - reward: 0.0154\n",
            "15 episodes - episode_reward: 10.267 [3.000, 25.000] - loss: 0.010 - mae: 0.530 - mean_q: 0.638 - mean_eps: 0.610 - ale.lives: 2.131\n",
            "\n",
            "Interval 34 (330000 steps performed)\n",
            "10000/10000 [==============================] - 696s 70ms/step - reward: 0.0189\n",
            "13 episodes - episode_reward: 13.692 [8.000, 23.000] - loss: 0.010 - mae: 0.547 - mean_q: 0.660 - mean_eps: 0.598 - ale.lives: 1.908\n",
            "\n",
            "Interval 35 (340000 steps performed)\n",
            "10000/10000 [==============================] - 889s 89ms/step - reward: 0.0192\n",
            "15 episodes - episode_reward: 13.933 [5.000, 29.000] - loss: 0.010 - mae: 0.559 - mean_q: 0.674 - mean_eps: 0.586 - ale.lives: 1.948\n",
            "\n",
            "Interval 36 (350000 steps performed)\n",
            "10000/10000 [==============================] - 935s 93ms/step - reward: 0.0167\n",
            "19 episodes - episode_reward: 8.684 [4.000, 17.000] - loss: 0.009 - mae: 0.571 - mean_q: 0.688 - mean_eps: 0.574 - ale.lives: 2.114\n",
            "\n",
            "Interval 37 (360000 steps performed)\n",
            "10000/10000 [==============================] - 600s 60ms/step - reward: 0.0172\n",
            "15 episodes - episode_reward: 11.400 [3.000, 32.000] - loss: 0.009 - mae: 0.578 - mean_q: 0.695 - mean_eps: 0.562 - ale.lives: 2.169\n",
            "\n",
            "Interval 38 (370000 steps performed)\n",
            "10000/10000 [==============================] - 842s 84ms/step - reward: 0.0187\n",
            "14 episodes - episode_reward: 13.286 [2.000, 32.000] - loss: 0.010 - mae: 0.591 - mean_q: 0.713 - mean_eps: 0.550 - ale.lives: 1.883\n",
            "\n",
            "Interval 39 (380000 steps performed)\n",
            "10000/10000 [==============================] - 779s 78ms/step - reward: 0.0194\n",
            "13 episodes - episode_reward: 13.769 [7.000, 23.000] - loss: 0.010 - mae: 0.612 - mean_q: 0.737 - mean_eps: 0.538 - ale.lives: 2.053\n",
            "\n",
            "Interval 40 (390000 steps performed)\n",
            "10000/10000 [==============================] - 796s 80ms/step - reward: 0.0184\n",
            "16 episodes - episode_reward: 12.750 [4.000, 23.000] - loss: 0.010 - mae: 0.631 - mean_q: 0.761 - mean_eps: 0.526 - ale.lives: 2.209\n",
            "\n",
            "Interval 41 (400000 steps performed)\n",
            "10000/10000 [==============================] - 803s 80ms/step - reward: 0.0183\n",
            "16 episodes - episode_reward: 11.062 [4.000, 21.000] - loss: 0.010 - mae: 0.639 - mean_q: 0.769 - mean_eps: 0.514 - ale.lives: 2.170\n",
            "\n",
            "Interval 42 (410000 steps performed)\n",
            "10000/10000 [==============================] - 833s 83ms/step - reward: 0.0186\n",
            "15 episodes - episode_reward: 12.333 [5.000, 21.000] - loss: 0.011 - mae: 0.663 - mean_q: 0.799 - mean_eps: 0.502 - ale.lives: 2.286\n",
            "\n",
            "Interval 43 (420000 steps performed)\n",
            "10000/10000 [==============================] - 818s 82ms/step - reward: 0.0160\n",
            "17 episodes - episode_reward: 9.471 [4.000, 15.000] - loss: 0.010 - mae: 0.670 - mean_q: 0.807 - mean_eps: 0.490 - ale.lives: 2.028\n",
            "\n",
            "Interval 44 (430000 steps performed)\n",
            "10000/10000 [==============================] - 826s 83ms/step - reward: 0.0185\n",
            "14 episodes - episode_reward: 12.857 [6.000, 19.000] - loss: 0.010 - mae: 0.673 - mean_q: 0.809 - mean_eps: 0.478 - ale.lives: 1.920\n",
            "\n",
            "Interval 45 (440000 steps performed)\n",
            "10000/10000 [==============================] - 988s 99ms/step - reward: 0.0179\n",
            "15 episodes - episode_reward: 12.267 [4.000, 23.000] - loss: 0.010 - mae: 0.687 - mean_q: 0.826 - mean_eps: 0.466 - ale.lives: 2.072\n",
            "\n",
            "Interval 46 (450000 steps performed)\n",
            "10000/10000 [==============================] - 807s 81ms/step - reward: 0.0190\n",
            "15 episodes - episode_reward: 12.733 [4.000, 24.000] - loss: 0.011 - mae: 0.715 - mean_q: 0.860 - mean_eps: 0.454 - ale.lives: 1.924\n",
            "\n",
            "Interval 47 (460000 steps performed)\n",
            "10000/10000 [==============================] - 803s 80ms/step - reward: 0.0180\n",
            "14 episodes - episode_reward: 12.786 [6.000, 28.000] - loss: 0.011 - mae: 0.720 - mean_q: 0.865 - mean_eps: 0.442 - ale.lives: 2.040\n",
            "\n",
            "Interval 48 (470000 steps performed)\n",
            "10000/10000 [==============================] - 881s 88ms/step - reward: 0.0189\n",
            "15 episodes - episode_reward: 13.000 [5.000, 23.000] - loss: 0.011 - mae: 0.736 - mean_q: 0.884 - mean_eps: 0.430 - ale.lives: 1.983\n",
            "\n",
            "Interval 49 (480000 steps performed)\n",
            "10000/10000 [==============================] - 990s 99ms/step - reward: 0.0212\n",
            "14 episodes - episode_reward: 14.500 [6.000, 23.000] - loss: 0.011 - mae: 0.752 - mean_q: 0.905 - mean_eps: 0.418 - ale.lives: 2.045\n",
            "\n",
            "Interval 50 (490000 steps performed)\n",
            "10000/10000 [==============================] - 1468s 147ms/step - reward: 0.0206\n",
            "14 episodes - episode_reward: 14.714 [7.000, 30.000] - loss: 0.011 - mae: 0.770 - mean_q: 0.925 - mean_eps: 0.406 - ale.lives: 1.988\n",
            "\n",
            "Interval 51 (500000 steps performed)\n",
            "10000/10000 [==============================] - 861s 86ms/step - reward: 0.0190\n",
            "13 episodes - episode_reward: 13.846 [9.000, 23.000] - loss: 0.011 - mae: 0.794 - mean_q: 0.954 - mean_eps: 0.394 - ale.lives: 1.997\n",
            "\n",
            "Interval 52 (510000 steps performed)\n",
            "10000/10000 [==============================] - 892s 89ms/step - reward: 0.0202\n",
            "12 episodes - episode_reward: 17.833 [9.000, 27.000] - loss: 0.011 - mae: 0.792 - mean_q: 0.951 - mean_eps: 0.382 - ale.lives: 2.039\n",
            "\n",
            "Interval 53 (520000 steps performed)\n",
            "10000/10000 [==============================] - 910s 91ms/step - reward: 0.0178\n",
            "13 episodes - episode_reward: 13.154 [4.000, 34.000] - loss: 0.011 - mae: 0.802 - mean_q: 0.964 - mean_eps: 0.370 - ale.lives: 2.111\n",
            "\n",
            "Interval 54 (530000 steps performed)\n",
            "10000/10000 [==============================] - 919s 92ms/step - reward: 0.0172\n",
            "16 episodes - episode_reward: 11.125 [4.000, 21.000] - loss: 0.011 - mae: 0.827 - mean_q: 0.993 - mean_eps: 0.358 - ale.lives: 1.967\n",
            "\n",
            "Interval 55 (540000 steps performed)\n",
            "10000/10000 [==============================] - 922s 92ms/step - reward: 0.0196\n",
            "16 episodes - episode_reward: 12.562 [7.000, 22.000] - loss: 0.010 - mae: 0.817 - mean_q: 0.981 - mean_eps: 0.346 - ale.lives: 2.045\n",
            "\n",
            "Interval 56 (550000 steps performed)\n",
            "10000/10000 [==============================] - 926s 93ms/step - reward: 0.0190\n",
            "10 episodes - episode_reward: 16.600 [8.000, 27.000] - loss: 0.010 - mae: 0.825 - mean_q: 0.989 - mean_eps: 0.334 - ale.lives: 1.845\n",
            "\n",
            "Interval 57 (560000 steps performed)\n",
            "10000/10000 [==============================] - 920s 92ms/step - reward: 0.0186\n",
            "13 episodes - episode_reward: 15.538 [8.000, 29.000] - loss: 0.010 - mae: 0.830 - mean_q: 0.996 - mean_eps: 0.322 - ale.lives: 2.024\n",
            "\n",
            "Interval 58 (570000 steps performed)\n",
            "10000/10000 [==============================] - 900s 90ms/step - reward: 0.0194\n",
            "14 episodes - episode_reward: 14.000 [8.000, 23.000] - loss: 0.010 - mae: 0.835 - mean_q: 1.002 - mean_eps: 0.310 - ale.lives: 2.254\n",
            "\n",
            "Interval 59 (580000 steps performed)\n",
            "10000/10000 [==============================] - 945s 94ms/step - reward: 0.0195\n",
            "11 episodes - episode_reward: 17.909 [7.000, 30.000] - loss: 0.010 - mae: 0.836 - mean_q: 1.004 - mean_eps: 0.298 - ale.lives: 2.179\n",
            "\n",
            "Interval 60 (590000 steps performed)\n",
            "10000/10000 [==============================] - 948s 95ms/step - reward: 0.0196\n",
            "14 episodes - episode_reward: 14.143 [6.000, 23.000] - loss: 0.011 - mae: 0.842 - mean_q: 1.009 - mean_eps: 0.286 - ale.lives: 2.107\n",
            "\n",
            "Interval 61 (600000 steps performed)\n",
            "10000/10000 [==============================] - 952s 95ms/step - reward: 0.0176\n",
            "13 episodes - episode_reward: 13.308 [5.000, 27.000] - loss: 0.011 - mae: 0.865 - mean_q: 1.037 - mean_eps: 0.274 - ale.lives: 2.162\n",
            "\n",
            "Interval 62 (610000 steps performed)\n",
            "10000/10000 [==============================] - 956s 96ms/step - reward: 0.0202\n",
            "15 episodes - episode_reward: 13.800 [6.000, 23.000] - loss: 0.011 - mae: 0.879 - mean_q: 1.054 - mean_eps: 0.262 - ale.lives: 2.130\n",
            "\n",
            "Interval 63 (620000 steps performed)\n",
            "10000/10000 [==============================] - 961s 96ms/step - reward: 0.0179\n",
            "12 episodes - episode_reward: 14.333 [5.000, 25.000] - loss: 0.011 - mae: 0.885 - mean_q: 1.061 - mean_eps: 0.250 - ale.lives: 2.000\n",
            "\n",
            "Interval 64 (630000 steps performed)\n",
            "10000/10000 [==============================] - 963s 96ms/step - reward: 0.0185\n",
            "14 episodes - episode_reward: 13.571 [4.000, 28.000] - loss: 0.011 - mae: 0.898 - mean_q: 1.076 - mean_eps: 0.238 - ale.lives: 2.084\n",
            "\n",
            "Interval 65 (640000 steps performed)\n",
            "10000/10000 [==============================] - 972s 97ms/step - reward: 0.0168\n",
            "13 episodes - episode_reward: 12.615 [4.000, 21.000] - loss: 0.011 - mae: 0.907 - mean_q: 1.087 - mean_eps: 0.226 - ale.lives: 1.910\n",
            "\n",
            "Interval 66 (650000 steps performed)\n",
            "10000/10000 [==============================] - 978s 98ms/step - reward: 0.0166\n",
            "14 episodes - episode_reward: 11.500 [2.000, 26.000] - loss: 0.011 - mae: 0.927 - mean_q: 1.110 - mean_eps: 0.214 - ale.lives: 2.009\n",
            "\n",
            "Interval 67 (660000 steps performed)\n",
            "10000/10000 [==============================] - 988s 99ms/step - reward: 0.0193\n",
            "15 episodes - episode_reward: 13.733 [5.000, 28.000] - loss: 0.011 - mae: 0.932 - mean_q: 1.117 - mean_eps: 0.202 - ale.lives: 2.087\n",
            "\n",
            "Interval 68 (670000 steps performed)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 991s 99ms/step - reward: 0.0199\n",
            "12 episodes - episode_reward: 15.667 [4.000, 21.000] - loss: 0.011 - mae: 0.952 - mean_q: 1.141 - mean_eps: 0.190 - ale.lives: 2.083\n",
            "\n",
            "Interval 69 (680000 steps performed)\n",
            "10000/10000 [==============================] - 998s 100ms/step - reward: 0.0167\n",
            "11 episodes - episode_reward: 14.636 [9.000, 23.000] - loss: 0.012 - mae: 0.958 - mean_q: 1.148 - mean_eps: 0.178 - ale.lives: 2.063\n",
            "\n",
            "Interval 70 (690000 steps performed)\n",
            "10000/10000 [==============================] - 1009s 101ms/step - reward: 0.0193\n",
            "13 episodes - episode_reward: 15.308 [7.000, 22.000] - loss: 0.012 - mae: 0.968 - mean_q: 1.160 - mean_eps: 0.166 - ale.lives: 2.002\n",
            "\n",
            "Interval 71 (700000 steps performed)\n",
            "10000/10000 [==============================] - 1020s 102ms/step - reward: 0.0174\n",
            "14 episodes - episode_reward: 13.143 [4.000, 25.000] - loss: 0.012 - mae: 0.976 - mean_q: 1.171 - mean_eps: 0.154 - ale.lives: 2.127\n",
            "\n",
            "Interval 72 (710000 steps performed)\n",
            "10000/10000 [==============================] - 1024s 102ms/step - reward: 0.0191\n",
            "12 episodes - episode_reward: 15.667 [7.000, 30.000] - loss: 0.012 - mae: 0.987 - mean_q: 1.183 - mean_eps: 0.142 - ale.lives: 1.982\n",
            "\n",
            "Interval 73 (720000 steps performed)\n",
            "10000/10000 [==============================] - 1034s 103ms/step - reward: 0.0164\n",
            "12 episodes - episode_reward: 14.000 [6.000, 19.000] - loss: 0.012 - mae: 1.010 - mean_q: 1.210 - mean_eps: 0.130 - ale.lives: 2.029\n",
            "\n",
            "Interval 74 (730000 steps performed)\n",
            "10000/10000 [==============================] - 1050s 105ms/step - reward: 0.0178\n",
            "14 episodes - episode_reward: 12.500 [8.000, 21.000] - loss: 0.012 - mae: 1.028 - mean_q: 1.232 - mean_eps: 0.118 - ale.lives: 2.146\n",
            "\n",
            "Interval 75 (740000 steps performed)\n",
            "10000/10000 [==============================] - 1057s 106ms/step - reward: 0.0178\n",
            "15 episodes - episode_reward: 11.467 [5.000, 21.000] - loss: 0.012 - mae: 1.041 - mean_q: 1.248 - mean_eps: 0.106 - ale.lives: 2.114\n",
            "\n",
            "Interval 76 (750000 steps performed)\n",
            "10000/10000 [==============================] - 1059s 106ms/step - reward: 0.0180\n",
            "12 episodes - episode_reward: 15.750 [9.000, 26.000] - loss: 0.012 - mae: 1.054 - mean_q: 1.264 - mean_eps: 0.100 - ale.lives: 2.096\n",
            "\n",
            "Interval 77 (760000 steps performed)\n",
            "10000/10000 [==============================] - 1061s 106ms/step - reward: 0.0175\n",
            "11 episodes - episode_reward: 15.364 [6.000, 31.000] - loss: 0.012 - mae: 1.049 - mean_q: 1.259 - mean_eps: 0.100 - ale.lives: 2.180\n",
            "\n",
            "Interval 78 (770000 steps performed)\n",
            "10000/10000 [==============================] - 1064s 106ms/step - reward: 0.0194\n",
            "13 episodes - episode_reward: 15.462 [6.000, 33.000] - loss: 0.012 - mae: 1.048 - mean_q: 1.257 - mean_eps: 0.100 - ale.lives: 1.896\n",
            "\n",
            "Interval 79 (780000 steps performed)\n",
            "10000/10000 [==============================] - 1065s 106ms/step - reward: 0.0186\n",
            "10 episodes - episode_reward: 16.700 [6.000, 34.000] - loss: 0.011 - mae: 1.053 - mean_q: 1.263 - mean_eps: 0.100 - ale.lives: 1.921\n",
            "\n",
            "Interval 80 (790000 steps performed)\n",
            "10000/10000 [==============================] - 1065s 106ms/step - reward: 0.0178\n",
            "13 episodes - episode_reward: 15.154 [7.000, 31.000] - loss: 0.011 - mae: 1.064 - mean_q: 1.276 - mean_eps: 0.100 - ale.lives: 2.247\n",
            "\n",
            "Interval 81 (800000 steps performed)\n",
            "10000/10000 [==============================] - 1070s 107ms/step - reward: 0.0179\n",
            "11 episodes - episode_reward: 15.091 [5.000, 25.000] - loss: 0.011 - mae: 1.068 - mean_q: 1.279 - mean_eps: 0.100 - ale.lives: 2.125\n",
            "\n",
            "Interval 82 (810000 steps performed)\n",
            "10000/10000 [==============================] - 1064s 106ms/step - reward: 0.0187\n",
            "13 episodes - episode_reward: 14.923 [5.000, 27.000] - loss: 0.011 - mae: 1.069 - mean_q: 1.282 - mean_eps: 0.100 - ale.lives: 1.958\n",
            "\n",
            "Interval 83 (820000 steps performed)\n",
            "10000/10000 [==============================] - 1078s 108ms/step - reward: 0.0175\n",
            "12 episodes - episode_reward: 14.583 [6.000, 24.000] - loss: 0.011 - mae: 1.091 - mean_q: 1.309 - mean_eps: 0.100 - ale.lives: 2.033\n",
            "\n",
            "Interval 84 (830000 steps performed)\n",
            "10000/10000 [==============================] - 1075s 107ms/step - reward: 0.0172\n",
            "15 episodes - episode_reward: 11.533 [7.000, 17.000] - loss: 0.012 - mae: 1.112 - mean_q: 1.333 - mean_eps: 0.100 - ale.lives: 2.099\n",
            "\n",
            "Interval 85 (840000 steps performed)\n",
            "10000/10000 [==============================] - 1071s 107ms/step - reward: 0.0172\n",
            "12 episodes - episode_reward: 13.000 [4.000, 22.000] - loss: 0.012 - mae: 1.135 - mean_q: 1.360 - mean_eps: 0.100 - ale.lives: 1.909\n",
            "\n",
            "Interval 86 (850000 steps performed)\n",
            "10000/10000 [==============================] - 1077s 108ms/step - reward: 0.0181\n",
            "14 episodes - episode_reward: 13.857 [4.000, 31.000] - loss: 0.013 - mae: 1.155 - mean_q: 1.383 - mean_eps: 0.100 - ale.lives: 2.240\n",
            "\n",
            "Interval 87 (860000 steps performed)\n",
            "10000/10000 [==============================] - 1071s 107ms/step - reward: 0.0200\n",
            "12 episodes - episode_reward: 16.750 [7.000, 26.000] - loss: 0.013 - mae: 1.162 - mean_q: 1.392 - mean_eps: 0.100 - ale.lives: 2.223\n",
            "\n",
            "Interval 88 (870000 steps performed)\n",
            "10000/10000 [==============================] - 1070s 107ms/step - reward: 0.0151\n",
            "10 episodes - episode_reward: 14.900 [6.000, 28.000] - loss: 0.012 - mae: 1.179 - mean_q: 1.412 - mean_eps: 0.100 - ale.lives: 1.909\n",
            "\n",
            "Interval 89 (880000 steps performed)\n",
            "10000/10000 [==============================] - 1080s 108ms/step - reward: 0.0161\n",
            "15 episodes - episode_reward: 11.333 [5.000, 22.000] - loss: 0.012 - mae: 1.168 - mean_q: 1.400 - mean_eps: 0.100 - ale.lives: 2.116\n",
            "\n",
            "Interval 90 (890000 steps performed)\n",
            "10000/10000 [==============================] - 1090s 109ms/step - reward: 0.0175\n",
            "10 episodes - episode_reward: 15.900 [8.000, 23.000] - loss: 0.012 - mae: 1.175 - mean_q: 1.408 - mean_eps: 0.100 - ale.lives: 2.122\n",
            "\n",
            "Interval 91 (900000 steps performed)\n",
            "10000/10000 [==============================] - 1086s 109ms/step - reward: 0.0196\n",
            "12 episodes - episode_reward: 16.500 [6.000, 29.000] - loss: 0.012 - mae: 1.169 - mean_q: 1.401 - mean_eps: 0.100 - ale.lives: 2.139\n",
            "\n",
            "Interval 92 (910000 steps performed)\n",
            "10000/10000 [==============================] - 1090s 109ms/step - reward: 0.0166\n",
            "12 episodes - episode_reward: 14.083 [7.000, 21.000] - loss: 0.012 - mae: 1.170 - mean_q: 1.402 - mean_eps: 0.100 - ale.lives: 2.034\n",
            "\n",
            "Interval 93 (920000 steps performed)\n",
            "10000/10000 [==============================] - 1081s 108ms/step - reward: 0.0165\n",
            "12 episodes - episode_reward: 13.583 [5.000, 28.000] - loss: 0.012 - mae: 1.183 - mean_q: 1.417 - mean_eps: 0.100 - ale.lives: 2.181\n",
            "\n",
            "Interval 94 (930000 steps performed)\n",
            "10000/10000 [==============================] - 1081s 108ms/step - reward: 0.0191\n",
            "15 episodes - episode_reward: 12.667 [6.000, 31.000] - loss: 0.012 - mae: 1.193 - mean_q: 1.429 - mean_eps: 0.100 - ale.lives: 2.182\n",
            "\n",
            "Interval 95 (940000 steps performed)\n",
            "10000/10000 [==============================] - 1194s 119ms/step - reward: 0.0194\n",
            "15 episodes - episode_reward: 13.733 [8.000, 27.000] - loss: 0.012 - mae: 1.217 - mean_q: 1.458 - mean_eps: 0.100 - ale.lives: 2.076\n",
            "\n",
            "Interval 96 (950000 steps performed)\n",
            "10000/10000 [==============================] - 1094s 109ms/step - reward: 0.0172\n",
            "12 episodes - episode_reward: 14.500 [5.000, 26.000] - loss: 0.012 - mae: 1.227 - mean_q: 1.470 - mean_eps: 0.100 - ale.lives: 2.079\n",
            "\n",
            "Interval 97 (960000 steps performed)\n",
            "10000/10000 [==============================] - 1084s 108ms/step - reward: 0.0163\n",
            "10 episodes - episode_reward: 16.300 [9.000, 24.000] - loss: 0.012 - mae: 1.243 - mean_q: 1.490 - mean_eps: 0.100 - ale.lives: 1.750\n",
            "\n",
            "Interval 98 (970000 steps performed)\n",
            "10000/10000 [==============================] - 1093s 109ms/step - reward: 0.0186\n",
            "12 episodes - episode_reward: 15.000 [6.000, 24.000] - loss: 0.012 - mae: 1.249 - mean_q: 1.498 - mean_eps: 0.100 - ale.lives: 2.118\n",
            "\n",
            "Interval 99 (980000 steps performed)\n",
            "10000/10000 [==============================] - 1090s 109ms/step - reward: 0.0202\n",
            "12 episodes - episode_reward: 16.500 [7.000, 25.000] - loss: 0.012 - mae: 1.260 - mean_q: 1.510 - mean_eps: 0.100 - ale.lives: 2.075\n",
            "\n",
            "Interval 100 (990000 steps performed)\n",
            "10000/10000 [==============================] - 1091s 109ms/step - reward: 0.0178\n",
            "done, took 85576.216 seconds\n"
          ]
        }
      ],
      "source": [
        "# Training part\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "checkpoint_weights_filename = 'dqn_' + env_name + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(env_name)\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=100000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "\n",
        "dqn.fit(env, callbacks=callbacks, nb_steps=1000000, log_interval=10000, visualize=False)\n",
        "\n",
        "dqn.save_weights(weights_filename, overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHYryKd1Gb2b",
        "outputId": "2b256978-b435-4c4f-f75f-0681f922d998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 668\n",
            "Episode 2: reward: 20.000, steps: 973\n",
            "Episode 3: reward: 11.000, steps: 621\n",
            "Episode 4: reward: 4.000, steps: 466\n",
            "Episode 5: reward: 14.000, steps: 877\n",
            "Episode 6: reward: 22.000, steps: 860\n",
            "Episode 7: reward: 9.000, steps: 643\n",
            "Episode 8: reward: 10.000, steps: 658\n",
            "Episode 9: reward: 16.000, steps: 884\n",
            "Episode 10: reward: 4.000, steps: 427\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x204b520c808>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR1FMJYMTfhd",
        "outputId": "2cc1d0ef-3991-4f11-e22f-d9adee9f66b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\david\\anaconda3\\envs\\VIU_Maestria\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: reward: 14.000, steps: 625\n",
            "Episode 2: reward: 29.000, steps: 1331\n",
            "Episode 3: reward: 15.000, steps: 618\n",
            "Episode 4: reward: 19.000, steps: 849\n",
            "Episode 5: reward: 21.000, steps: 1144\n",
            "Episode 6: reward: 15.000, steps: 955\n",
            "Episode 7: reward: 20.000, steps: 1229\n",
            "Episode 8: reward: 23.000, steps: 1019\n",
            "Episode 9: reward: 10.000, steps: 572\n",
            "Episode 10: reward: 20.000, steps: 905\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x22fb76cf2c8>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights_900000.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxErWIfITfhe",
        "outputId": "dea747ef-5d90-485c-cfd5-0adf33b9a3dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n",
            "Episode 1: reward: 31.000, steps: 1291\n",
            "Episode 2: reward: 24.000, steps: 1040\n",
            "Episode 3: reward: 21.000, steps: 907\n",
            "Episode 4: reward: 20.000, steps: 1054\n",
            "Episode 5: reward: 22.000, steps: 944\n",
            "Episode 6: reward: 21.000, steps: 1004\n",
            "Episode 7: reward: 29.000, steps: 1706\n",
            "Episode 8: reward: 28.000, steps: 1169\n",
            "Episode 9: reward: 28.000, steps: 1376\n",
            "Episode 10: reward: 19.000, steps: 927\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x22fb76bd7c8>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Testing part to calculate the mean reward\n",
        "weights_filename = 'dqn_{}_weights_900000.h5f'.format(env_name)\n",
        "dqn.load_weights(weights_filename)\n",
        "dqn.test(env, nb_episodes=10, visualize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NAlu8b1Gb2b"
      },
      "source": [
        "### 3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El juego Space Invaders es un juego clásico de la consola Atari, para automatizar la jugabilidad del juego hemos implementado una solución basada en aprendizaje por refuerzo, esta solución se ha implementado con Deep Q-learning haciendo uso de una red CNN (Convolutional Neuronal Network), ya que lo que deseamos interpretar son las imagenes que representan diferentes escenarios donde va interactuar nuestro agente.\n",
        "\n",
        "Todas las imagenes del juego tiene una imagen original de 210x160 pixeles en canal RGB, por ende una dimensión de 210x160x3.\n",
        "\n",
        "Con el fin de reducir el tiempo computacional del entrenamiento en nuestro modelo y la complejidad de los estados se realizan unas cuentas modificaciones a los datos de entrada, en este caso las imagenes del juego, estás se listan a continuación:\n",
        "\n",
        "1.   Reducir la cantidad de pixeles por imagen a 84x84, esto nos permitira  \n",
        "     tener menos pesos parámetrizables en nuestra CNN.\n",
        "2.   Reducir a un canal, escala de grises las imagenes, ya que el color no\n",
        "     representa mayor información para el aprendizaje\n",
        "3.   Dividir las imagenes entre 255 para normalizar los valores de lopixeles,   esto acorta la complejidad de trabajar con un rango mayor de valores.\n",
        "4.   \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EtQbCvthUWrQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANFQiicXK3sO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sV4pacYXtxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}